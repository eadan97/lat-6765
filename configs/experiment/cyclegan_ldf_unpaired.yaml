# @package _global_

# to execute this experiment run:
# python run.py experiment=example_simple.yaml

defaults:
  - override /trainer: sb3.yaml # choose trainer from 'configs/trainer/'
  - override /model: cyclegan_model.yaml
  - override /datamodule: cyclegan_datamodule.yaml
  - override /callbacks: null
  - override /logger: wandb.yaml

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

seed: 12345

trainer:
  check_val_every_n_epoch: 10
  max_epochs: 100

model:
  lr_g: 2e-4
  lr_d: 2e-4
  lambda_perception: 0.05


callbacks:
  model_checkpoint:
    _target_: pytorch_lightning.callbacks.ModelCheckpoint
    monitor: "val/FID"
    save_top_k: 1
    save_last: True
    mode: "min"
    dirpath: "checkpoints/"
    filename: "{epoch:02d}"
    save_on_train_epoch_end: False

  early_stopping:
    _target_: pytorch_lightning.callbacks.EarlyStopping
    monitor: "val/FID"
    patience: 100
    mode: "min"
    check_on_train_epoch_end: False

  upload_ckpts_as_artifact:
    _target_: src.callbacks.wandb_callbacks.UploadCheckpointsAsArtifact
    ckpt_dir: "checkpoints/"
    upload_best_only: False


datamodule:
  batch_size: 1
  train_val_test_split: [ 0.9, 0.1 ]
  dataset_name: "ldf"
  restricted: True

logger:
  wandb:
    group: "cyclegan"
    tags: [ "cyclegan", "unpaired", "ldf", "90/10" ]
